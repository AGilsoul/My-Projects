# CS-Portfolio
Project Descriptions:

HIGHLIGHTED PROJECTS - These are the projects that I am most proud of, the descriptions of which can be found with a ** next to them, most of which are C++. My favorite and most complex projects are my C++ NeuralNetwork and Python matrixNeuralNet.


C++:

**  KNearestNeighbors: While doing research on different machine learning algorithms, I noticed that most machine learning libraries were written only for Python. Noting that, I challenged myself to write a machine learning algorithm ,"K-Nearest-Neighbors" or KNN, from scratch using only C++. This program takes the data from a csv file of data for over 500 breast cancer tumors, and based on the data, determines whether a tumor is malignant or benign. The program creates "points" from each tumor, and places the first 95% points (practice points) from the dataset and graphs them on a normalized scale. Then, it tests itself by attempting to classify the last 5% of points (test points) from the dataset. It does this by taking the Euclidean distance for the data columns of each test point to all of the practice points. Then, based on a specified value "k", it takes the labels of the "k" nearest points, and determines the category (malignant/benign) of the test point based on which label has more instances within the "k" nearest points. In the program's current state, it classify tumors with an average of ~96-97% accuracy. While this program is not perfect, it shows the potential of machine learning (and more sophisticated algorithms) in the future and how it can be used in healthcare.
  
**  KMeansClustering: Another machine learning algorithm that I coded from scratch in C++. This program is similar to KNN, but rather than classifying points based on every training point, centroids are created and centered in data clusters. Whichever label is most common in that data cluster is what the centroid is then classified as, and the label of the closest centroid to the point to be classified is what the label of the point is predicted to be.
  
**  LinearRegression: Another machine learning algorithm that I coded from scratch in C++. This project contains a class that can fit a line, plane, or hyperplane to a set of data using regression techniques, and contains a main.cpp file for an example run. The adjustments to the coefficients of the line/plane are calculated using gradient descent and partial derivatives.

***  NeuralNetwork: This project is also from made from scratch in C++. The network adjusts the weights and biases of connections between structs called neurons to determine probabilities, such as the probability of a written character being a certain number, or of a cat being in a picture. There are mulitple run configurations in the main method, for multiple datasets for use in tasks such as breast tumor malignancy recognition, handwritten digit recognition, the prediction of residential structure cooling energy loads, and more. The "test_<datasetname>_config" functions load a pre-trained neural network, and test it, while the <datasetname>_config create a new neural network, train it, and save the data in a csv file. The neural networks can be trained on the breast cancer dataset in a reasonable amount of time (< 1 minute), but for the full MNIST handwritten digit dataset, it can take at least an hour to train using stochastic gradient descent (dataset includes 60,000 samples with 784 features each), so I recommend that you load the pre-trained model instead. The network can also use min-batch gradient descent, and while the accuracy suffers a little, the training time is reduced to under 10 minutes in most cases, a huge improvement time wise (it is still a work in progres, I am fine tuning to find the best training configuration for mini-batch for optimal accuracy). On both datasets, the networks accuracy sits comfortably in the 97%-100% range using stochastic gradient descent, with 100% accuracy not being uncommon. The model performs well on regression tasks as well, but not quite as well as it does on classification tasks due to the fact that there is no clear right or wrong with regression, however, comparisons with data targets and data predictions show a good fit with the model where there is correlation between the data

  2420_FinalProject: In this project, I coded a binary tree object, and then created a method for balancing the tree to optimize average query time for each node. The user first specifies the number of nodes they want in the tree. The program then creates a tree with all numbers up to that number, inserting them in a random order. The program then balances the tree, and displays a percentage difference in the average node distance between the balanced and unbalanced trees.
  
  1410(FinalProject): In this project, I made a command line program that utilized a CSV file containing all production cars sold in the United States in 2020 to find the best car for a user to fit their needs based on certain preferances such as build, mileage, drivetrain, and so on. It implements a selection sort to sort cars based on horsepower, mileage, and price. The user is greated by a menu from which they can filter the list, sort the list, and do a google search for the cars currently in the list.
  
  
 Python:

  K-Means Clustering: In this program, I coded my own K-Means Clustering machine learning algorithm to classify three different types of iris flowers based on the length of their sepal and the width of their sepal. The program moves three different points, or centroids around the graph based on the data provided to locate them in the center of the three different flower clusters, and colors the flower data points based on the nearest centroid.

  BreastCancerClassifier: In this program, I used the K-Nearest Neighbors machine learning algorithm from the sklearn library to determine whether patients have breast cancer based on a myriad of factors that were recorded for each patient. The algorithm is then tested, and a graph is shown displaying its accuracy on a validation set of patients with a different number of "K-Nearest Neighbors". It can be seen that the accuracy of the algorithm reaches about 96.5% when K is set to 10. The algorithm can be tested on a different arrangment of the data by changing the random state in the code.
  
  ***matrixNeuralNet: This is my second neural network that I've written from scratch, only this time in Python. I utilized the NumPy library to simplify the code and by using matrix dot products to find the weighted sum of neurons for an entire layer rather than having to do so individually. This not only makes the code itself shorter, but increases the readability. It has been tested on one dataset so far that includes data on residential structures, and can predict the cooling energy load of these structures with an R^2 value of up to 0.96.
